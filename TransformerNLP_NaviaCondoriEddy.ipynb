{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaviaEddy/SegundoParcial_SIS421/blob/main/TransformerNLP_NaviaCondoriEddy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI9RLzsIl2f9",
        "outputId": "c1b0c0a0-c98e-4531-f3a3-d1e95ba218e9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.1)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "J2OLFIMeeU2o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim #que proporciona varios algoritmos de optimización (como SGD, Adam, etc.)\n",
        "                            # utilizados para actualizar los pesos de la red neuronal durante el entrenamiento\n",
        "\n",
        "import spacy #es una biblioteca de procesamiento de lenguaje natural (NLP)\n",
        "             # para cargar modelos de lenguaje, tokenizar texto, etc.\n",
        "\n",
        "import warnings #que permite emitir advertencias en Python.\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter # se utiliza para escribir datos de entrenamiento (como la pérdida y la precisión) en un archivo\n",
        "\n",
        "from sklearn.model_selection import train_test_split #Esta función se utiliza para dividir un conjunto de datos\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence # se utiliza para rellenar secuencias de diferentes longitudes a la misma longitud\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Supresión de advertencias específicas\n",
        "warnings.filterwarnings(\"ignore\", message=\"enable_nested_tensor is True, but self.use_nested_tensor is False because\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"The verbose parameter is deprecated.\")"
      ],
      "metadata": {
        "id": "IQtgVwDt4-6x"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "RXmtqz3orSDP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Descargamos el dataset\n",
        "!wget https://www.manythings.org/anki/spa-eng.zip -O spa-eng.zip\n",
        "!unzip spa-eng.zip -d spa-eng\n",
        "\n",
        "# Cargamos el dataset\n",
        "data_path = 'spa-eng/spa.txt'\n",
        "\n",
        "# Lee el conjunto de datos y conserva sólo las dos primeras columnas\n",
        "df = pd.read_csv(data_path, delimiter='\\t', header=None, names=['src', 'trg', '_extra1', '_extra2', '_extra3'])\n",
        "\n",
        "# Eliminar las columnas adicionales\n",
        "df = df[['src', 'trg']]\n",
        "\n",
        "# Mostramos las filas\n",
        "print(df.head())\n",
        "\n",
        "# Guardamos el dataset tratado\n",
        "df.to_csv('spa-eng/spa_processed.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "0KTVwZlpmjUd",
        "outputId": "b4bcc19b-e7be-400b-e842-317a1d23c937"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "A UTF-8 locale is required. Got ANSI_X3.4-1968",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-f9b8b7ae089a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Descargamos el dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wget https://www.manythings.org/anki/spa-eng.zip -O spa-eng.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unzip spa-eng.zip -d spa-eng'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    455\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocale_encoding\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_ENCODING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m       raise NotImplementedError(\n\u001b[0m\u001b[1;32m    169\u001b[0m           \u001b[0;34m'A UTF-8 locale is required. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocale_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m       )\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: A UTF-8 locale is required. Got ANSI_X3.4-1968"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga los datos desde un archivo CSV\n",
        "df = pd.read_csv('spa-eng/spa_processed.csv')\n",
        "\n",
        "# Extrae todas las frases en español de la columna 'src' del DataFrame y las convierte en una lista de cadenas de texto.\n",
        "src_sentences = df['src'].tolist()\n",
        "\n",
        "# Extrae todas las frases en inglés de la columna 'trg' del DataFrame y las convierte en una lista de cadenas de texto.\n",
        "trg_sentences = df['trg'].tolist()"
      ],
      "metadata": {
        "id": "qp1CotFF3vyC"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def translate_sentence(model, sentence, src_vocab, trg_vocab, reverse_trg_vocab, device, max_length=50):\n",
        "    # Carga el modelo de spaCy para el idioma español\n",
        "    spacy_es = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "    # Tokeniza la oración si es una cadena de texto\n",
        "    if isinstance(sentence, str):\n",
        "        # Convierte la oración en una lista de tokens en minúsculas\n",
        "        tokens = [token.text.lower() for token in spacy_es(sentence)]\n",
        "    else:\n",
        "        # Si la oración ya es una lista de tokens, solo los convierte a minúsculas\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    # Añade los tokens de inicio de oración (SOS) y fin de oración (EOS)\n",
        "    tokens.insert(0, \"<sos>\")\n",
        "    tokens.append(\"<eos>\")\n",
        "\n",
        "    # Convierte los tokens en índices según el vocabulario de origen, si no se\n",
        "    # encuentra la palabra devuelve <unk>\n",
        "    text_to_indices = [src_vocab.get(token, src_vocab[\"<unk>\"]) for token in tokens]\n",
        "\n",
        "    # Convierte la lista de índices en un tensor y lo añade una dimensión extra para batch\n",
        "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
        "\n",
        "    # Inicializa la lista de salidas con el token de inicio de oración del vocabulario de destino\n",
        "    outputs = [trg_vocab[\"<sos>\"]]\n",
        "\n",
        "    # Traduce la oración token por token hasta el máximo de longitud permitido\n",
        "    for _ in range(max_length):\n",
        "        # Convierte la lista de salidas en un tensor y lo añade una dimensión extra para batch\n",
        "        trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
        "\n",
        "        # Desactiva el cálculo de gradientes para ahorrar memoria y acelerar la inferencia\n",
        "        with torch.no_grad():\n",
        "            # Pasa el tensor de la oración y el tensor de las salidas al modelo para obtener el siguiente token\n",
        "            output = model(sentence_tensor, trg_tensor)\n",
        "\n",
        "        # Selecciona el índice del token con la mayor probabilidad en la última predicción\n",
        "        best_guess = output.argmax(2)[-1, :].item()\n",
        "\n",
        "        # Añade el índice del mejor token a la lista de salidas\n",
        "        outputs.append(best_guess)\n",
        "\n",
        "        # Si el token de fin de oración (EOS) es predicho, se detiene la traducción\n",
        "        if best_guess == trg_vocab[\"<eos>\"]:\n",
        "            break\n",
        "\n",
        "    # Convierte los índices a sentencias\n",
        "    translated_sentence = [reverse_trg_vocab[token] for token in outputs[1:]]\n",
        "\n",
        "    # Retorna la oración traducida como una lista de tokens\n",
        "    return translated_sentence\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n"
      ],
      "metadata": {
        "id": "eyQSYOk2pbqr"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descarga y carga de modelos de spacy\n",
        "spacy.require_gpu()  # Indica a spacy que use la GPU si está disponible para acelerar el procesamiento\n",
        "spacy_es = spacy.load(\"es_core_news_sm\")  # Carga el modelo de spacy para procesar texto en español\n",
        "spacy_en = spacy.load(\"en_core_web_sm\")  # Carga el modelo de spacy para procesar texto en inglés\n",
        "\n",
        "# Definición de funciones de tokenización\n",
        "def tokenize_es(text):\n",
        "    # Tokeniza el texto en español utilizando el modelo de spacy para español\n",
        "    return [tok.text for tok in spacy_es.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    # Tokeniza el texto en inglés utilizando el modelo de spacy para inglés\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "metadata": {
        "id": "zp4NvqdX3mXc"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definición de un dataset personalizado para traducción\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src_sentences, trg_sentences, src_tokenizer, trg_tokenizer, src_vocab, trg_vocab):\n",
        "        # Inicializa el dataset con las oraciones de origen y destino, así como los tokenizadores y vocabularios correspondientes\n",
        "        self.src_sentences = src_sentences  # Lista de oraciones en el idioma de origen\n",
        "        self.trg_sentences = trg_sentences  # Lista de oraciones en el idioma de destino\n",
        "        self.src_tokenizer = src_tokenizer  # Función para tokenizar oraciones en el idioma de origen\n",
        "        self.trg_tokenizer = trg_tokenizer  # Función para tokenizar oraciones en el idioma de destino\n",
        "        self.src_vocab = src_vocab  # Vocabulario para el idioma de origen (diccionario de token a índice)\n",
        "        self.trg_vocab = trg_vocab  # Vocabulario para el idioma de destino (diccionario de token a índice)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Devuelve el número de ejemplos en el dataset\n",
        "        return len(self.src_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Devuelve el par de tokens (oración de origen y oración de destino) en forma de tensores para un índice dado\n",
        "        # Tokeniza la oración de origen y la convierte en índices según el vocabulario\n",
        "        src_tokens = [self.src_vocab.get(token, self.src_vocab[\"<unk>\"]) for token in self.src_tokenizer(self.src_sentences[idx])]\n",
        "        # Tokeniza la oración de destino y la convierte en índices según el vocabulario\n",
        "        trg_tokens = [self.trg_vocab.get(token, self.trg_vocab[\"<unk>\"]) for token in self.trg_tokenizer(self.trg_sentences[idx])]\n",
        "        # Convierte las listas de índices en tensores y los devuelve como una tupla\n",
        "        return torch.tensor(src_tokens), torch.tensor(trg_tokens)"
      ],
      "metadata": {
        "id": "fC02Yss63n9k"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(tokenizer, sentences, min_freq=2):\n",
        "    # Inicializa un diccionario vacío para almacenar las frecuencias de los tokens\n",
        "    vocab = {}\n",
        "    # Itera sobre cada oración en el conjunto de datos\n",
        "    for sentence in sentences:\n",
        "        # Tokeniza la oración usando el tokenizador proporcionado\n",
        "        tokens = tokenizer(sentence)\n",
        "        # Itera sobre cada token en la oración\n",
        "        for token in tokens:\n",
        "            # Incrementa la frecuencia del token en el vocabulario\n",
        "            if token in vocab:\n",
        "                vocab[token] += 1\n",
        "            else:\n",
        "                vocab[token] = 1\n",
        "    # Filtra el vocabulario para mantener solo los tokens que ocurren al menos min_freq veces\n",
        "    vocab = {k: v for k, v in vocab.items() if v >= min_freq}\n",
        "    # Asigna un índice único a cada token en el vocabulario, comenzando desde 4\n",
        "    vocab = {token: idx for idx, token in enumerate(vocab.keys(), start=4)}\n",
        "    # Agrega tokens especiales al vocabulario con índices predefinidos\n",
        "    vocab[\"<unk>\"] = 0  # Token para palabras desconocidas\n",
        "    vocab[\"<pad>\"] = 1  # Token de relleno para hacer que las secuencias tengan la misma longitud\n",
        "    vocab[\"<sos>\"] = 2  # Token de inicio de secuencia\n",
        "    vocab[\"<eos>\"] = 3  # Token de final de secuencia\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "VR2xnoIh3plb"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separamos los\n",
        "train_src, test_src, train_trg, test_trg = train_test_split(src_sentences, trg_sentences, test_size=0.1)\n",
        "train_src, val_src, train_trg, val_trg = train_test_split(train_src, train_trg, test_size=0.1)\n",
        "\n",
        "# Construimos los vocabularios\n",
        "src_vocab = build_vocab(tokenize_es, train_src)\n",
        "trg_vocab = build_vocab(tokenize_en, train_trg)\n",
        "\n",
        "# Cambia de indices a sentencias\n",
        "reverse_trg_vocab = {v: k for k, v in trg_vocab.items()}\n",
        "\n",
        "# Creamos el dataset\n",
        "train_dataset = TranslationDataset(train_src, train_trg, tokenize_es, tokenize_en, src_vocab, trg_vocab)\n",
        "val_dataset = TranslationDataset(val_src, val_trg, tokenize_es, tokenize_en, src_vocab, trg_vocab)\n",
        "test_dataset = TranslationDataset(test_src, test_trg, tokenize_es, tokenize_en, src_vocab, trg_vocab)\n",
        "\n",
        "# Se encarga de manejar que todos los lotes tengan el mismo len agregando un pad\n",
        "def collate_fn(batch):\n",
        "    src_batch, trg_batch = zip(*batch)\n",
        "    src_batch = pad_sequence(src_batch, padding_value=src_vocab[\"<pad>\"])\n",
        "    trg_batch = pad_sequence(trg_batch, padding_value=trg_vocab[\"<pad>\"])\n",
        "    return src_batch, trg_batch\n",
        "\n",
        "# Creamos los dataloaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "1h4P0Fd630HD"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embedding_size, src_vocab_size, trg_vocab_size, src_pad_idx, num_heads, num_encoder_layers, num_decoder_layers, forward_expansion, dropout, max_len, device):\n",
        "        super(Transformer, self).__init__()\n",
        "        #Esta capa se utiliza para convertir los índices de las palabras en vectores de embedding(eng).\n",
        "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
        "        #Esta capa se utiliza para representar la posición de cada palabra en la secuencia.\n",
        "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
        "        #Esta capa se utiliza para convertir los índices de las palabras en vectores de embedding(esp).\n",
        "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
        "        #Esta capa se utiliza para representar la posición de cada palabra en la secuencia.\n",
        "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
        "        self.device = device\n",
        "        # Capa principal del Transformer\n",
        "        self.transformer = nn.Transformer(embedding_size, num_heads, num_encoder_layers, num_decoder_layers, forward_expansion, dropout)\n",
        "        #Crea una capa lineal que se utilizará como capa de salida del modelo.\n",
        "        #Esta capa transforma la salida del Transformer en una representación adecuada para predecir las palabras en la secuencia objetivo.\n",
        "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.src_pad_idx = src_pad_idx #indice de padding\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        src_mask = src.transpose(0, 1) == self.src_pad_idx #Verifica que tokens llevan pad(true) y no pad(false)\n",
        "        return src_mask.to(self.device)\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        # (seq len, emdeding)\n",
        "        src_seq_length, N = src.shape\n",
        "        # (seq len, emdeding)\n",
        "        trg_seq_length, N = trg.shape\n",
        "        # Genera un tensor de posiciones para la fuente con una secuencia de 0 hasta seq len y añadiendole una dimension extra\n",
        "        # para que este en el formato (seq len, emdeding)\n",
        "        src_positions = torch.arange(0, src_seq_length).unsqueeze(1).expand(src_seq_length, N).to(self.device)\n",
        "        trg_positions = torch.arange(0, trg_seq_length).unsqueeze(1).expand(trg_seq_length, N).to(self.device)\n",
        "        #Suma el embeding y la posicion de representacion de embeding para cada fuente\n",
        "        embed_src = self.dropout((self.src_word_embedding(src) + self.src_position_embedding(src_positions)))\n",
        "        embed_trg = self.dropout((self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions)))\n",
        "        #Se ocupa de ocultar el pad\n",
        "        src_padding_mask = self.make_src_mask(src)\n",
        "        #Evita que el modelo atienda los pad de los trg finales\n",
        "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(self.device)\n",
        "        #Se pasa las secuencias fuente y objetivo, junto con las máscaras para controlar la informacion\n",
        "        out = self.transformer(embed_src, embed_trg, src_key_padding_mask=src_padding_mask, tgt_mask=trg_mask)\n",
        "        #Obtenemos la distribucion de probabilidades para la prediccion\n",
        "        out = self.fc_out(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "npTF2Fz24Mp5"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "load_model = True\n",
        "save_model = True\n",
        "num_epochs = 40\n",
        "learning_rate = 3e-4\n",
        "src_vocab_size = len(src_vocab)\n",
        "trg_vocab_size = len(trg_vocab)\n",
        "embedding_size = 512\n",
        "num_heads = 8\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "dropout = 0.10\n",
        "max_len = 100\n",
        "forward_expansion = 4\n",
        "src_pad_idx = trg_vocab[\"<pad>\"]"
      ],
      "metadata": {
        "id": "NlsavYIo3_nE"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Transformer(embedding_size, src_vocab_size, trg_vocab_size, src_pad_idx, num_heads, num_encoder_layers, num_decoder_layers, forward_expansion, dropout, max_len, device).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=10, verbose=True)\n",
        "pad_idx = trg_vocab[\"<pad>\"]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
      ],
      "metadata": {
        "id": "7yrMfwrW4K1D"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"my_checkpoint.pth.tar\"\n",
        "if load_model and os.path.exists(checkpoint_path):\n",
        "    load_checkpoint(torch.load(checkpoint_path), model, optimizer)\n",
        "\n",
        "sentence = \"i want to go your favorite place please\"\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "\n",
        "    if save_model:\n",
        "        checkpoint = {\n",
        "            \"state_dict\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "        }\n",
        "        save_checkpoint(checkpoint, filename=checkpoint_path)\n",
        "\n",
        "    model.eval()\n",
        "    translated_sentence = translate_sentence(model, sentence, src_vocab, trg_vocab, reverse_trg_vocab, device, max_length=30)\n",
        "    print(f\"Ejemplos de traducciones: \\n {' '.join(translated_sentence)}\")\n",
        "    model.train()\n",
        "\n",
        "    train_losses = []\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        inp_data, target = batch\n",
        "        inp_data = inp_data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        output = model(inp_data, target[:-1, :])\n",
        "        output = output.reshape(-1, output.shape[2])\n",
        "        target = target[1:].reshape(-1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, target)\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = output.max(1)\n",
        "        train_total += target.size(0)\n",
        "        train_correct += (predicted == target).sum().item()\n",
        "\n",
        "    mean_train_loss = sum(train_losses) / len(train_losses)\n",
        "    train_acc = 100 * train_correct / train_total\n",
        "\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(val_loader):\n",
        "            inp_data, target = batch\n",
        "            inp_data = inp_data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            output = model(inp_data, target[:-1, :])\n",
        "            output = output.reshape(-1, output.shape[2])\n",
        "            target = target[1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "            val_losses.append(loss.item())\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            _, predicted = output.max(1)\n",
        "            val_total += target.size(0)\n",
        "            val_correct += (predicted == target).sum().item()\n",
        "\n",
        "    mean_val_loss = sum(val_losses) / len(val_losses)\n",
        "    val_acc = 100 * val_correct / val_total\n",
        "\n",
        "    scheduler.step(mean_val_loss)\n",
        "\n",
        "    print(f\"Train Loss: {mean_train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"Val Loss: {mean_val_loss:.4f}, Val Acc: {val_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydEpEJ3WpXnL",
        "outputId": "fcea41f7-8d7e-4581-be5e-ccbc6f0b5840"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 0 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " imaginar chicas imaginar vecino convertirme criticado planeo imaginar criticado convertirme tienden interesan planeo planeo planeo planeo planeo planeo planeo planeo planeo planeo reaccionado literatura Les interesan imaginar habló Ponle reaccionado\n",
            "Train Loss: 3.6514, Train Acc: 18.79%\n",
            "Val Loss: 2.3822, Val Acc: 24.57%\n",
            "[Epoch 1 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " que quieres ir tu lugar , por favor . No <unk> . No ! <unk> . No . No . No . No ! <unk> . No . No .\n",
            "Train Loss: 2.2836, Train Acc: 24.87%\n",
            "Val Loss: 1.9515, Val Acc: 26.94%\n",
            "[Epoch 2 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " de <unk> que vaya tu lugar por favor . No <unk> más de <unk> . No <unk> . No . No por favor . No . No . No <unk>\n",
            "Train Loss: 1.8748, Train Acc: 26.89%\n",
            "Val Loss: 1.7790, Val Acc: 28.26%\n",
            "[Epoch 3 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quiere ir tu sitio sitio por favor . A veces . A que <unk> . No lugar . A . A . A . A miedo hogar miedo lugar\n",
            "Train Loss: 1.6508, Train Acc: 28.12%\n",
            "Val Loss: 1.6964, Val Acc: 28.84%\n",
            "[Epoch 4 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " que quieres ir tu lugar de por favor , por favor . <unk> . <unk> . <unk> . <unk> . <unk> por favor por favor . Tienes permiso . Tienes\n",
            "Train Loss: 1.5099, Train Acc: 29.09%\n",
            "Val Loss: 1.6362, Val Acc: 29.45%\n",
            "[Epoch 5 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quiero ir tu lugar de tu lugar de que <unk> por favor . Sin detalles . Sin . Sin . Sin . Sin . Sin lugar . Sin .\n",
            "Train Loss: 1.4010, Train Acc: 29.77%\n",
            "Val Loss: 1.5857, Val Acc: 29.93%\n",
            "[Epoch 6 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> querría ir tu sitio , por favor . No , por favor <unk> . No . No . No . No favor . No favor ' . No .\n",
            "Train Loss: 1.3132, Train Acc: 30.42%\n",
            "Val Loss: 1.5652, Val Acc: 30.07%\n",
            "[Epoch 7 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quiero ir tu casa , por favor . <unk> la naturaleza . <unk> . <unk> . <unk> . <unk> . <unk> . <unk> . <unk> . <unk> . <unk>\n",
            "Train Loss: 1.2449, Train Acc: 30.97%\n",
            "Val Loss: 1.5429, Val Acc: 30.41%\n",
            "[Epoch 8 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieren ir tu lugar de tu lugar de <unk> por favor . <unk> . Vas . Vas . ¡ <unk> . <unk> . ¡ . <unk> . ¡ lugar\n",
            "Train Loss: 1.1899, Train Acc: 31.40%\n",
            "Val Loss: 1.5074, Val Acc: 30.73%\n",
            "[Epoch 9 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> querés ir tu lugar de lugar por favor , por favor . <unk> . Quiero ir . Alguien . Alguien <unk> . <unk> <unk> . Alguien <unk> <unk> .\n",
            "Train Loss: 1.1412, Train Acc: 31.66%\n",
            "Val Loss: 1.5148, Val Acc: 30.69%\n",
            "[Epoch 10 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " de <unk> quieren ir tu coche favorito por favor de las estaciones a caballo . Quiere a caballo . Vas a tu noche por <unk> a tu próximo por favor\n",
            "Train Loss: 1.0982, Train Acc: 32.07%\n",
            "Val Loss: 1.4990, Val Acc: 30.93%\n",
            "[Epoch 11 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quiere ir a tu lugar favorito por favor <unk> . Quiero ir . Quiero ir . lugar <unk> . Quiero <unk> <unk> . Quiero , por tu lugar .\n",
            "Train Loss: 1.0602, Train Acc: 32.42%\n",
            "Val Loss: 1.4839, Val Acc: 31.12%\n",
            "[Epoch 12 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieren ir tu lugar favorito , por favor . <unk> , por favor . <unk> . <unk> . <unk> . Te . Te . Te <unk> . Te .\n",
            "Train Loss: 1.0257, Train Acc: 32.74%\n",
            "Val Loss: 1.4898, Val Acc: 31.22%\n",
            "[Epoch 13 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieren ir tu lugar más verde por favor , por favor . <unk> por favor . <unk> . <unk> . <unk> . <unk> . <unk> <unk> que <unk> .\n",
            "Train Loss: 0.9933, Train Acc: 32.97%\n",
            "Val Loss: 1.4664, Val Acc: 31.24%\n",
            "[Epoch 14 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieres ir tu lugar favorito por favor . Quiero letras <unk> . Quiero ir a alza . \" . \" . \" Quiero . \" . \" \" .\n",
            "Train Loss: 0.9679, Train Acc: 33.15%\n",
            "Val Loss: 1.4669, Val Acc: 31.53%\n",
            "[Epoch 15 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieren ir tu lugar favorito por favor . Quiero ir a ir a tu lugar . <unk> . Tengo <unk> . No lugar por Internet lugar a Internet .\n",
            "Train Loss: 0.9428, Train Acc: 33.37%\n",
            "Val Loss: 1.4660, Val Acc: 31.55%\n",
            "[Epoch 16 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieres ir a tu sitio favorito por favor . No se ríe de experiencia . ¡ . Quiero . Quiero . Quiero ir tus <unk> . Quiero . Quiero\n",
            "Train Loss: 0.9168, Train Acc: 33.65%\n",
            "Val Loss: 1.4539, Val Acc: 31.63%\n",
            "[Epoch 17 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quiero ir a tu lugar preferido por favor . Me <unk> . Me por favor . Me . Me . Me las las <unk> . Me las <unk> .\n",
            "Train Loss: 0.8952, Train Acc: 33.89%\n",
            "Val Loss: 1.4567, Val Acc: 31.57%\n",
            "[Epoch 18 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieren ir tu lugar favorito por favor . <unk> ' veces . <unk> ' ir . Por favor . Por detalles . favorito . Por , por . Por\n",
            "Train Loss: 0.8751, Train Acc: 34.03%\n",
            "Val Loss: 1.4480, Val Acc: 31.73%\n",
            "[Epoch 19 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieren ir a tu lugar preferido por favor . <unk> , por favor <unk> , <unk> , por favor <unk> , por <unk> , por favor , por ,\n",
            "Train Loss: 0.8536, Train Acc: 34.19%\n",
            "Val Loss: 1.4476, Val Acc: 31.75%\n",
            "[Epoch 20 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieren ir a tu lugar favorito por favor . <unk> porque se <unk> por favor <unk> <unk> <unk> . <unk> . <unk> . <unk> . <unk> tus <unk> .\n",
            "Train Loss: 0.8377, Train Acc: 34.37%\n",
            "Val Loss: 1.4479, Val Acc: 31.88%\n",
            "[Epoch 21 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> de tu lugar favorito por favor . <unk> detalles por favor . <unk> . <unk> . Tengo . <unk> . <unk> . <unk> . <unk> . <unk> . <unk>\n",
            "Train Loss: 0.8200, Train Acc: 34.55%\n",
            "Val Loss: 1.4586, Val Acc: 31.81%\n",
            "[Epoch 22 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieres ir tu lugar favorito , por favor . Por favor <unk> . Por favor . Por . Por . Por . Por . Por Por . <unk> .\n",
            "Train Loss: 0.8039, Train Acc: 34.69%\n",
            "Val Loss: 1.4379, Val Acc: 31.94%\n",
            "[Epoch 23 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " quieras ir tu lugar favorito por favor . Por favor . <unk> . Por favor . <unk> . Por . <unk> <unk> . <unk> favor . <unk> . <unk> .\n",
            "Train Loss: 0.7865, Train Acc: 34.75%\n",
            "Val Loss: 1.4424, Val Acc: 31.94%\n",
            "[Epoch 24 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieren ir a tu casa , por favor . <unk> detalles . Por favor . Te <unk> <unk> . Te <unk> . Me . Por . Te . Te\n",
            "Train Loss: 0.7731, Train Acc: 34.93%\n",
            "Val Loss: 1.4376, Val Acc: 32.02%\n",
            "[Epoch 25 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieres ir a tu casa favorito por favor . Las <unk> . Las alguna vez . Las <unk> . Las las Las hoja . Las donde corbata , <unk>\n",
            "Train Loss: 0.7586, Train Acc: 35.16%\n",
            "Val Loss: 1.4359, Val Acc: 32.10%\n",
            "[Epoch 26 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieres ir tu lugar preferido por favor . <unk> . Me soldados . \" <unk> . Me . '' . '' <unk> . '' . '' . '' .\n",
            "Train Loss: 0.7426, Train Acc: 35.27%\n",
            "Val Loss: 1.4356, Val Acc: 32.10%\n",
            "[Epoch 27 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieres ir tu casa favorito por favor . <unk> <unk> . Me <unk> . Por favor <unk> . <unk> . <unk> . <unk> . Me . <unk> . <unk>\n",
            "Train Loss: 0.7315, Train Acc: 35.46%\n",
            "Val Loss: 1.4387, Val Acc: 32.10%\n",
            "[Epoch 28 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieres ir tu lugar favorito por favor . Por naturaleza . Por naturaleza . Por favor <unk> . Por . Por naturaleza naturaleza <unk> . Por <unk> . Nadie\n",
            "Train Loss: 0.7232, Train Acc: 35.44%\n",
            "Val Loss: 1.4335, Val Acc: 32.18%\n",
            "[Epoch 29 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieres ir tu lugar favorito , por favor . Por favor <unk> <unk> <unk> <unk> <unk> . Por naturaleza naturaleza . Por naturaleza . Por Por naturaleza naturaleza .\n",
            "Train Loss: 0.7122, Train Acc: 35.57%\n",
            "Val Loss: 1.4375, Val Acc: 32.24%\n",
            "[Epoch 30 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> querés ir a tu casa preferido por favor . Por favor . <unk> ' ir a ir de empleados . Por . <unk> . <unk> . empleados . Por\n",
            "Train Loss: 0.6972, Train Acc: 35.61%\n",
            "Val Loss: 1.4378, Val Acc: 32.36%\n",
            "[Epoch 31 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieren ir tus sitios por favor hágamelo <unk> . Por favor . <unk> . Por favor . Por naturaleza <unk> . Por <unk> . Por <unk> . Me <unk>\n",
            "Train Loss: 0.6871, Train Acc: 35.79%\n",
            "Val Loss: 1.4472, Val Acc: 32.27%\n",
            "[Epoch 32 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieres ir a tu lugar favorito por favor . <unk> las ocho . Por favor . Por . <unk> . Por favor <unk> . Por <unk> . <unk> .\n",
            "Train Loss: 0.6791, Train Acc: 35.92%\n",
            "Val Loss: 1.4411, Val Acc: 32.31%\n",
            "[Epoch 33 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieres ir a tu lugar favorito por favor . <unk> . \" <unk> . No . No . No . No , Me . No . No . <unk>\n",
            "Train Loss: 0.6657, Train Acc: 35.99%\n",
            "Val Loss: 1.4514, Val Acc: 32.38%\n",
            "[Epoch 34 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " compasión quieres ir tu lugar favorito por favor . <unk> . Por favor , por favor . Me . '' . '' . por '' . Por favor . Por\n",
            "Train Loss: 0.6587, Train Acc: 36.10%\n",
            "Val Loss: 1.4292, Val Acc: 32.39%\n",
            "[Epoch 35 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quiero ir a tu lugar favorito por favor . <unk> . <unk> . <unk> . Por favor . <unk> <unk> . Por Por . Por <unk> . Por favor\n",
            "Train Loss: 0.6473, Train Acc: 36.13%\n",
            "Val Loss: 1.4337, Val Acc: 32.41%\n",
            "[Epoch 36 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieren ir a tu casa favorito por favor . <unk> ' favor . Por favor . Por . ' . . Por ' . Por . '' . Por\n",
            "Train Loss: 0.6381, Train Acc: 36.26%\n",
            "Val Loss: 1.4529, Val Acc: 32.35%\n",
            "[Epoch 37 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quiero ir tu lugar favorito por favor . Las <unk> . Por favor . Por . Por favor . Por . Por . Me Tengo Tengo Tengo . Por\n",
            "Train Loss: 0.6310, Train Acc: 36.23%\n",
            "Val Loss: 1.4460, Val Acc: 32.42%\n",
            "[Epoch 38 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieres ir tu lugar favorito por favor . <unk> <unk> <unk> . Por favor <unk> . <unk> . <unk> . '' . <unk> <unk> <unk> . Por . <unk>\n",
            "Train Loss: 0.6221, Train Acc: 36.41%\n",
            "Val Loss: 1.4582, Val Acc: 32.48%\n",
            "[Epoch 39 / 40]\n",
            "=> Saving checkpoint\n",
            "Ejemplos de traducciones: \n",
            " <unk> quieren ir tu lugar preferido por favor . <unk> letras de <unk> . Por favor . Por favor . Por . Por '' . Por favor . Por Por\n",
            "Train Loss: 0.6163, Train Acc: 36.55%\n",
            "Val Loss: 1.4550, Val Acc: 32.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el checkpoint\n",
        "checkpoint_path = \"my_checkpoint.pth.tar\"\n",
        "if os.path.exists(checkpoint_path):\n",
        "    load_checkpoint(torch.load(checkpoint_path), model, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-2o19QYO8SI",
        "outputId": "c38b479d-f1a3-4332-ecd9-d75092002719"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Loading checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frase para traducir\n",
        "sentence = \"i like you\"\n",
        "\n",
        "# Realizar la traducción\n",
        "model.eval()\n",
        "translated_sentence = translate_sentence(model, sentence, src_vocab, trg_vocab, reverse_trg_vocab, device, max_length=10)\n",
        "translated_text = ' '.join(translated_sentence)\n",
        "print(f\"Traducción: {translated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epo3Zxl1ZpHm",
        "outputId": "1aa1507b-494d-421d-98d5-5b02412d7d51"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traducción: <unk> como te <unk> de que te <unk> a las\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}